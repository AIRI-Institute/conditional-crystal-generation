{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a965848-86fd-481b-b348-c56fba38dfcc",
   "metadata": {},
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78cbf336",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import joblib\n",
    "from IPython.display import clear_output\n",
    "from pymatgen.core import Structure\n",
    "\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from diffusers import DDIMScheduler\n",
    "\n",
    "from src.model.models import CrystalUNetModel\n",
    "from src.generation.generation import generate_diffusion\n",
    "from src.inference.inference_data_generation import generate_inference_dataset\n",
    "from src.utils import seed_everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3d8b4d8-a412-46ab-be70-cc24d81911a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class InferenceConfig:\n",
    "    # Data\n",
    "    max_nsites = 64\n",
    "    max_elems = 4\n",
    "    min_elems = 2\n",
    "\n",
    "    # Model\n",
    "    model_channels: int = 128\n",
    "    num_res_blocks: int = 7\n",
    "    attention_resolutions=(1, 2, 4, 8)\n",
    "    \n",
    "    # Noise Scheduler\n",
    "    num_train_timesteps = 1_000\n",
    "    num_inference_steps = 100\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    beta_schedule = \"squaredcos_cap_v2\" \n",
    "    clip_sample = False\n",
    "\n",
    "    # Training\n",
    "    batch_size = 256\n",
    "    num_workers = 1\n",
    "\n",
    "    # Accelerator\n",
    "    mixed_precision = 'fp16'  # `no` for float32, `fp16` for automatic mixed precision\n",
    "\n",
    "    device = \"cuda\"\n",
    "    random_state = 42 \n",
    "\n",
    "\n",
    "config = InferenceConfig()\n",
    "seed_everything(config.random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a24fc04-7e91-4dc2-bfa4-3a549160e211",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PATH = \"../FTCP_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "402bdcd8-9d39-4f3a-b589-0d06b9b8a0d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(    pretty_formula  spacegroup_relax  enthalpy_formation_atom\n",
       " 0          Ta1W1B6                 6                  -1.3993\n",
       " 1          Ta1W1B6                 6                  -1.4093\n",
       " 2          Ta1W1B6                 6                  -1.4193\n",
       " 3          Ta1W1B6                 6                  -1.4293\n",
       " 4          Ta1W1B6                 6                  -1.4393\n",
       " ..             ...               ...                      ...\n",
       " 394        Ta1W1B6               225                  -1.5593\n",
       " 395        Ta1W1B6               225                  -1.5693\n",
       " 396        Ta1W1B6               225                  -1.5793\n",
       " 397        Ta1W1B6               225                  -1.5893\n",
       " 398        Ta1W1B6               225                  -1.5993\n",
       " \n",
       " [399 rows x 3 columns],\n",
       " <src.inference.inference_data_generation.InferenceCrystalDataset at 0x7f441d115350>,\n",
       " 399)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spgs = [  6,   8,  10,  12,  25,  35,  44,  47,  65,  71,  99, 119, 123, 129, 139, 160, 166, 216, 225]\n",
    "formula = \"Ta1W1B6\"\n",
    "step = -0.01\n",
    "start = -0.3993 - 1\n",
    "n = 20\n",
    "\n",
    "df, inferece_dataset = generate_inference_dataset(\n",
    "        formula,\n",
    "        spgs,\n",
    "        step,\n",
    "        start,\n",
    "        n,\n",
    "        return_df=True,\n",
    "        data_path=\"../src/data/\"\n",
    ")\n",
    "\n",
    "df, inferece_dataset, len(inferece_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f2dd42d-daad-4e45-92e9-a9be5b3517db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = DataLoader(inferece_dataset, batch_size=config.batch_size, num_workers=config.num_workers, shuffle=False)\n",
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb3a456",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29fd89b4-7387-4891-86ba-d1cc5060ed00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrystalUNetModel(\n",
       "  (model): UNetModel(\n",
       "    (time_embed): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (label_emb): Linear(in_features=513, out_features=512, bias=True)\n",
       "    (input_blocks): ModuleList(\n",
       "      (0): TimestepEmbedSequential(\n",
       "        (0): Conv1d(3, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "      (1-7): 7 x TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "          (skip_connection): Identity()\n",
       "        )\n",
       "        (1): AttentionBlock(\n",
       "          (norm): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "          (qkv): Conv1d(128, 384, kernel_size=(1,), stride=(1,))\n",
       "          (attention): QKVAttention()\n",
       "          (proj_out): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (8): TimestepEmbedSequential(\n",
       "        (0): Downsample(\n",
       "          (op): Conv1d(128, 128, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "        )\n",
       "      )\n",
       "      (9): TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "          (skip_connection): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): AttentionBlock(\n",
       "          (norm): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (qkv): Conv1d(256, 768, kernel_size=(1,), stride=(1,))\n",
       "          (attention): QKVAttention()\n",
       "          (proj_out): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (10-15): 6 x TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "          (skip_connection): Identity()\n",
       "        )\n",
       "        (1): AttentionBlock(\n",
       "          (norm): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (qkv): Conv1d(256, 768, kernel_size=(1,), stride=(1,))\n",
       "          (attention): QKVAttention()\n",
       "          (proj_out): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (16): TimestepEmbedSequential(\n",
       "        (0): Downsample(\n",
       "          (op): Conv1d(256, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "        )\n",
       "      )\n",
       "      (17): TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv1d(256, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "          (skip_connection): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): AttentionBlock(\n",
       "          (norm): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "          (qkv): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))\n",
       "          (attention): QKVAttention()\n",
       "          (proj_out): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (18-23): 6 x TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "          (skip_connection): Identity()\n",
       "        )\n",
       "        (1): AttentionBlock(\n",
       "          (norm): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "          (qkv): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))\n",
       "          (attention): QKVAttention()\n",
       "          (proj_out): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (24): TimestepEmbedSequential(\n",
       "        (0): Downsample(\n",
       "          (op): Conv1d(512, 512, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "        )\n",
       "      )\n",
       "      (25): TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "          (skip_connection): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): AttentionBlock(\n",
       "          (norm): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
       "          (qkv): Conv1d(1024, 3072, kernel_size=(1,), stride=(1,))\n",
       "          (attention): QKVAttention()\n",
       "          (proj_out): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (26-31): 6 x TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "          (skip_connection): Identity()\n",
       "        )\n",
       "        (1): AttentionBlock(\n",
       "          (norm): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
       "          (qkv): Conv1d(1024, 3072, kernel_size=(1,), stride=(1,))\n",
       "          (attention): QKVAttention()\n",
       "          (proj_out): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (middle_block): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        )\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
       "        (qkv): Conv1d(1024, 3072, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttention()\n",
       "        (proj_out): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (2): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        )\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "    )\n",
       "    (output_blocks): ModuleList(\n",
       "      (0-6): 7 x TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 2048, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv1d(2048, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "          (skip_connection): Conv1d(2048, 1024, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): AttentionBlock(\n",
       "          (norm): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
       "          (qkv): Conv1d(1024, 3072, kernel_size=(1,), stride=(1,))\n",
       "          (attention): QKVAttention()\n",
       "          (proj_out): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (7): TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 1536, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv1d(1536, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "          (skip_connection): Conv1d(1536, 1024, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): AttentionBlock(\n",
       "          (norm): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
       "          (qkv): Conv1d(1024, 3072, kernel_size=(1,), stride=(1,))\n",
       "          (attention): QKVAttention()\n",
       "          (proj_out): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (2): Upsample(\n",
       "          (conv): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        )\n",
       "      )\n",
       "      (8): TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 1536, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv1d(1536, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "          (skip_connection): Conv1d(1536, 512, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): AttentionBlock(\n",
       "          (norm): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "          (qkv): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))\n",
       "          (attention): QKVAttention()\n",
       "          (proj_out): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (9-14): 6 x TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "          (skip_connection): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): AttentionBlock(\n",
       "          (norm): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "          (qkv): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))\n",
       "          (attention): QKVAttention()\n",
       "          (proj_out): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (15): TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv1d(768, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "          (skip_connection): Conv1d(768, 512, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): AttentionBlock(\n",
       "          (norm): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "          (qkv): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))\n",
       "          (attention): QKVAttention()\n",
       "          (proj_out): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (2): Upsample(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        )\n",
       "      )\n",
       "      (16): TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv1d(768, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "          (skip_connection): Conv1d(768, 256, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): AttentionBlock(\n",
       "          (norm): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (qkv): Conv1d(256, 768, kernel_size=(1,), stride=(1,))\n",
       "          (attention): QKVAttention()\n",
       "          (proj_out): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (17-22): 6 x TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv1d(512, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "          (skip_connection): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): AttentionBlock(\n",
       "          (norm): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (qkv): Conv1d(256, 768, kernel_size=(1,), stride=(1,))\n",
       "          (attention): QKVAttention()\n",
       "          (proj_out): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (23): TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv1d(384, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "          (skip_connection): Conv1d(384, 256, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): AttentionBlock(\n",
       "          (norm): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (qkv): Conv1d(256, 768, kernel_size=(1,), stride=(1,))\n",
       "          (attention): QKVAttention()\n",
       "          (proj_out): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (2): Upsample(\n",
       "          (conv): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        )\n",
       "      )\n",
       "      (24): TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv1d(384, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "          (skip_connection): Conv1d(384, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): AttentionBlock(\n",
       "          (norm): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "          (qkv): Conv1d(128, 384, kernel_size=(1,), stride=(1,))\n",
       "          (attention): QKVAttention()\n",
       "          (proj_out): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (25-31): 7 x TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "          (skip_connection): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): AttentionBlock(\n",
       "          (norm): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "          (qkv): Conv1d(128, 384, kernel_size=(1,), stride=(1,))\n",
       "          (attention): QKVAttention()\n",
       "          (proj_out): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (out): Sequential(\n",
       "      (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "      (1): SiLU()\n",
       "      (2): Conv1d(128, 3, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    )\n",
       "  )\n",
       "  (spg_condition): Sequential(\n",
       "    (0): Conv2d(192, 64, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "    (2): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (3): LeakyReLU(negative_slope=0.2)\n",
       "    (4): Conv2d(128, 256, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (5): LeakyReLU(negative_slope=0.2)\n",
       "    (6): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (element_condition): Sequential(\n",
       "    (0): Conv1d(125, 64, kernel_size=(3,), stride=(1,))\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Conv1d(64, 128, kernel_size=(3,), stride=(1,))\n",
       "    (3): GELU(approximate='none')\n",
       "    (4): Conv1d(128, 256, kernel_size=(3,), stride=(1,))\n",
       "    (5): GELU(approximate='none')\n",
       "    (6): Flatten(start_dim=1, end_dim=-1)\n",
       "    (7): Linear(in_features=14848, out_features=256, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CrystalUNetModel(\n",
    "    in_channels=3, # should be equal to num_features (input features) \n",
    "    dims=1, #this states, that we are using 1D U-Net\n",
    "    condition_dims=1 + 256 + 256, # num_condition_features 256 - is size of spacegroups condition\n",
    "    model_channels=config.model_channels, # inner model features\n",
    "    out_channels=3, # should be equal to num_features (input features) \n",
    "    num_res_blocks=config.num_res_blocks, # idk\n",
    "    attention_resolutions=config.attention_resolutions\n",
    ")\n",
    "model.to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f53118bd-eca0-4877-8948-d98edf2d9926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"<PATH TO YOU WEIGHTS>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1f147df-5e1f-492f-901c-4a58b810aeca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DDIMScheduler {\n",
       "  \"_class_name\": \"DDIMScheduler\",\n",
       "  \"_diffusers_version\": \"0.23.1\",\n",
       "  \"beta_end\": 0.02,\n",
       "  \"beta_schedule\": \"squaredcos_cap_v2\",\n",
       "  \"beta_start\": 0.0001,\n",
       "  \"clip_sample\": false,\n",
       "  \"clip_sample_range\": 1.0,\n",
       "  \"dynamic_thresholding_ratio\": 0.995,\n",
       "  \"num_train_timesteps\": 1000,\n",
       "  \"prediction_type\": \"epsilon\",\n",
       "  \"rescale_betas_zero_snr\": false,\n",
       "  \"sample_max_value\": 1.0,\n",
       "  \"set_alpha_to_one\": true,\n",
       "  \"steps_offset\": 0,\n",
       "  \"thresholding\": false,\n",
       "  \"timestep_spacing\": \"leading\",\n",
       "  \"trained_betas\": null\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddim_scheduler = DDIMScheduler(\n",
    "    num_train_timesteps=config.num_train_timesteps,\n",
    "    beta_start=config.beta_start,\n",
    "    beta_end=config.beta_end,\n",
    "    beta_schedule=config.beta_schedule,\n",
    "    clip_sample=config.clip_sample\n",
    ")\n",
    "ddim_scheduler.set_timesteps(\n",
    "    num_inference_steps=config.num_inference_steps\n",
    ")\n",
    "\n",
    "ddim_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8c97eda-27a2-4409-8929-6bb10fd3d852",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator(\n",
    "    mixed_precision=config.mixed_precision,\n",
    ")\n",
    "\n",
    "dataloader, model = accelerator.prepare(\n",
    "    dataloader, model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1518fbdb-5456-46b0-bda6-30958d025e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:19<00:00,  9.72s/it]\n"
     ]
    }
   ],
   "source": [
    "lattice_size = 3\n",
    "\n",
    "atoms_generated = []\n",
    "\n",
    "lattice_generated = []\n",
    "\n",
    "n_sites_dataset = []\n",
    "conditions = []\n",
    "\n",
    "elements_dataset = []\n",
    "\n",
    "x1_energies = []\n",
    "\n",
    "\n",
    "model.eval()\n",
    "for batch in tqdm(dataloader):\n",
    "    # get needed features\n",
    "    element_matrix = batch[\"element_matrix\"]\n",
    "    elemental_property_matrix = batch[\"elemental_property_matrix\"]\n",
    "    spg = batch[\"spg\"]\n",
    "\n",
    "    x1_energy = batch[\"energy\"]\n",
    "    condition = batch[\"energy\"]\n",
    "    n_sites = batch[\"n_sites\"]\n",
    "    (\n",
    "        element_matrix,\n",
    "        elemental_property_matrix,\n",
    "        condition,\n",
    "        spg,\n",
    "    ) = (\n",
    "        element_matrix.to(config.device),\n",
    "        elemental_property_matrix.to(config.device),\n",
    "        condition.to(config.device),\n",
    "        spg.to(config.device),\n",
    "    )\n",
    "    \n",
    "    x_0_coords = torch.rand((element_matrix.shape[0], 64, 3)).to(config.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = generate_diffusion(\n",
    "            model=model, \n",
    "            x_0=x_0_coords,\n",
    "            elements=torch.cat([element_matrix, elemental_property_matrix], dim=-1), \n",
    "            y=condition, \n",
    "            spg=spg,\n",
    "            noise_scheduler=ddim_scheduler\n",
    "        )\n",
    "        output = output.cpu()\n",
    "        coords_pred, lattice_pred = output[:, :-4], output[:, -3:]\n",
    "\n",
    "    atoms_generated.append(coords_pred.cpu())\n",
    "    lattice_generated.append(lattice_pred.cpu())\n",
    "    n_sites_dataset.append(n_sites.cpu())\n",
    "    conditions.append(condition.cpu())\n",
    "    elements_dataset.append(element_matrix)\n",
    "    x1_energies.append(x1_energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cc114a4-d34b-4f95-a17e-0ed062281e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "atoms_generated = torch.vstack(atoms_generated).detach().cpu().numpy()\n",
    "lattice_generated = torch.vstack(lattice_generated).detach().cpu().numpy()\n",
    "n_sites_dataset = torch.cat(n_sites_dataset).detach().cpu().numpy()\n",
    "conditions_dataset = torch.cat(conditions).detach().cpu().numpy()\n",
    "\n",
    "elements_dataset = torch.vstack(elements_dataset).detach().cpu().numpy()\n",
    "x1_energies = torch.cat(x1_energies).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba2668f1-0246-4920-8042-bd5b2614bf62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fbe30ac11c341db9ec7d2252e97b6e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/399 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.auto import tqdm\n",
    "from pymatgen.core import Structure\n",
    "\n",
    "elm_str = np.array(joblib.load(\"../src/data/element.pkl\"))\n",
    "\n",
    "\n",
    "def form_up_structure(one_hot_vectors, coordinates_input, lattice):\n",
    "    pred_elm = np.argmax(one_hot_vectors, axis=1)\n",
    "    pred_elm = elm_str[pred_elm]\n",
    "    struct = Structure(lattice=lattice, species=pred_elm, coords=coordinates_input)\n",
    "    return struct\n",
    "\n",
    "\n",
    "indexes_to_make = np.arange(0, len(atoms_generated))\n",
    "n_jobs = -1\n",
    "\n",
    "pred_structures = Parallel(n_jobs=n_jobs)(\n",
    "    delayed(form_up_structure)(\n",
    "        elements_dataset[i, : n_sites_dataset[i]],\n",
    "        atoms_generated[i][: n_sites_dataset[i]],\n",
    "        lattice_generated[i],\n",
    "    )\n",
    "    for i in tqdm(indexes_to_make)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e9d0794-8063-4e84-baf1-00c603887e76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# generated using pymatgen\n",
      "data_TaB6W\n",
      "_symmetry_space_group_name_H-M   'P 1'\n",
      "_cell_length_a   84.44050025\n",
      "_cell_length_b   90.96421129\n",
      "_cell_length_c   112.80858442\n",
      "_cell_angle_alpha   19.73437124\n",
      "_cell_angle_beta   25.54271838\n",
      "_cell_angle_gamma   40.66772462\n",
      "_symmetry_Int_Tables_number   1\n",
      "_chemical_formula_structural   TaB6W\n",
      "_chemical_formula_sum   'Ta1 B6 W1'\n",
      "_cell_volume   98635.82983580\n",
      "_cell_formula_units_Z   1\n",
      "loop_\n",
      " _symmetry_equiv_pos_site_id\n",
      " _symmetry_equiv_pos_as_xyz\n",
      "  1  'x, y, z'\n",
      "loop_\n",
      " _atom_site_type_symbol\n",
      " _atom_site_label\n",
      " _atom_site_symmetry_multiplicity\n",
      " _atom_site_fract_x\n",
      " _atom_site_fract_y\n",
      " _atom_site_fract_z\n",
      " _atom_site_occupancy\n",
      "  Ta  Ta0  1  28.53800774  68.27568817  24.53317833  1\n",
      "  W  W1  1  28.09791565  49.11555481  27.25648117  1\n",
      "  B  B2  1  68.66297150  7.88978481  60.80498123  1\n",
      "  B  B3  1  43.98616028  26.24205208  42.04769516  1\n",
      "  B  B4  1  49.02616882  53.97908020  12.62097168  1\n",
      "  B  B5  1  5.96825743  13.09373665  65.89971161  1\n",
      "  B  B6  1  24.76815987  45.53620148  57.88580322  1\n",
      "  B  B7  1  66.97824097  29.21427155  70.73694611  1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cifs = [structure.to(fmt='cif')  for structure in pred_structures]\n",
    "print(cifs[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65c1bcc7-7d63-4e4c-b797-40aaaa453e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pretty_formula</th>\n",
       "      <th>spacegroup_relax</th>\n",
       "      <th>enthalpy_formation_atom</th>\n",
       "      <th>cif</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ta1W1B6</td>\n",
       "      <td>6</td>\n",
       "      <td>-1.3993</td>\n",
       "      <td># generated using pymatgen\\ndata_TaB6W\\n_symme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ta1W1B6</td>\n",
       "      <td>6</td>\n",
       "      <td>-1.4093</td>\n",
       "      <td># generated using pymatgen\\ndata_TaB6W\\n_symme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ta1W1B6</td>\n",
       "      <td>6</td>\n",
       "      <td>-1.4193</td>\n",
       "      <td># generated using pymatgen\\ndata_TaB6W\\n_symme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ta1W1B6</td>\n",
       "      <td>6</td>\n",
       "      <td>-1.4293</td>\n",
       "      <td># generated using pymatgen\\ndata_TaB6W\\n_symme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ta1W1B6</td>\n",
       "      <td>6</td>\n",
       "      <td>-1.4393</td>\n",
       "      <td># generated using pymatgen\\ndata_TaB6W\\n_symme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>Ta1W1B6</td>\n",
       "      <td>225</td>\n",
       "      <td>-1.5593</td>\n",
       "      <td># generated using pymatgen\\ndata_TaB6W\\n_symme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>Ta1W1B6</td>\n",
       "      <td>225</td>\n",
       "      <td>-1.5693</td>\n",
       "      <td># generated using pymatgen\\ndata_TaB6W\\n_symme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>Ta1W1B6</td>\n",
       "      <td>225</td>\n",
       "      <td>-1.5793</td>\n",
       "      <td># generated using pymatgen\\ndata_TaB6W\\n_symme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>Ta1W1B6</td>\n",
       "      <td>225</td>\n",
       "      <td>-1.5893</td>\n",
       "      <td># generated using pymatgen\\ndata_TaB6W\\n_symme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>Ta1W1B6</td>\n",
       "      <td>225</td>\n",
       "      <td>-1.5993</td>\n",
       "      <td># generated using pymatgen\\ndata_TaB6W\\n_symme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>399 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    pretty_formula  spacegroup_relax  enthalpy_formation_atom  \\\n",
       "0          Ta1W1B6                 6                  -1.3993   \n",
       "1          Ta1W1B6                 6                  -1.4093   \n",
       "2          Ta1W1B6                 6                  -1.4193   \n",
       "3          Ta1W1B6                 6                  -1.4293   \n",
       "4          Ta1W1B6                 6                  -1.4393   \n",
       "..             ...               ...                      ...   \n",
       "394        Ta1W1B6               225                  -1.5593   \n",
       "395        Ta1W1B6               225                  -1.5693   \n",
       "396        Ta1W1B6               225                  -1.5793   \n",
       "397        Ta1W1B6               225                  -1.5893   \n",
       "398        Ta1W1B6               225                  -1.5993   \n",
       "\n",
       "                                                   cif  \n",
       "0    # generated using pymatgen\\ndata_TaB6W\\n_symme...  \n",
       "1    # generated using pymatgen\\ndata_TaB6W\\n_symme...  \n",
       "2    # generated using pymatgen\\ndata_TaB6W\\n_symme...  \n",
       "3    # generated using pymatgen\\ndata_TaB6W\\n_symme...  \n",
       "4    # generated using pymatgen\\ndata_TaB6W\\n_symme...  \n",
       "..                                                 ...  \n",
       "394  # generated using pymatgen\\ndata_TaB6W\\n_symme...  \n",
       "395  # generated using pymatgen\\ndata_TaB6W\\n_symme...  \n",
       "396  # generated using pymatgen\\ndata_TaB6W\\n_symme...  \n",
       "397  # generated using pymatgen\\ndata_TaB6W\\n_symme...  \n",
       "398  # generated using pymatgen\\ndata_TaB6W\\n_symme...  \n",
       "\n",
       "[399 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"cif\"] = cifs\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "513879e9-e3b5-4863-aadf-d5467888fe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"<FILENAME>.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
